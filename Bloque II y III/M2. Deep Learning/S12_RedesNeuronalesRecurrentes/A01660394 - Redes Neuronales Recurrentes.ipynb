{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOtfsgF8Z9xL"
      },
      "source": [
        "# Actividad: Redes Neuronales Profundas\n",
        "\n",
        "> Alfonso Pineda Cedillo | A01660394\n",
        "\n",
        "**Fecha de entrega:** 7 de Noviembre de 2023\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKDwVN3JZ9xM"
      },
      "source": [
        "**Instrucciones:**\n",
        "\n",
        " - Deberás de entrenar la red utilizando un libro en formato txt del tema de tu elección.\n",
        " - Muestra capturas de pantalla con resultados de 2 entradas y 3 temperaturas diferentes (6 resultados en total)\n",
        " - Sube un pdf donde muestres tu código y los resultados, además de mostrar el nombre o tema del libro utilizado para entrenar la red."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smx9jrpcZ9xM"
      },
      "source": [
        "## Solución\n",
        "\n",
        "En primer instancia, importamos las librerías necesarias para el desarrollo de la actividad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kcdA2c32Z9xN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPjoPRmOZ9xN"
      },
      "source": [
        "### Importación de texto de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdg-n0ESZ9xN"
      },
      "source": [
        "Para la actividad actual, se seleccionó el libro \"A Game of Thrones\" de George R. R. Martin, el cual se encuentra en formato txt. Para importar el texto, se utilizó la librería `urlib` y se guardó en una variable llamada `text`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip3IQOxqZ9xO",
        "outputId": "3b099996-4cc9-48c1-f6b2-a37cc89ff110"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longitud del texto: 5662324 caracteres\n",
            "Caracteres diferentes: 86\n",
            "Vocabulario:  ['\\n', ' ', '!', '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', 'é', 'ê', '—', '‘', '’', '“', '”', '…']\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/nihitx/game-of-thrones-/master/gameofthrones.txt'\n",
        "path_to_file = 'gameofthrones.txt'  # Nombre del archivo local en el que se guardará el texto descargado\n",
        "\n",
        "# Descargar el archivo de la URL\n",
        "urllib.request.urlretrieve(url, path_to_file)\n",
        "\n",
        "# Leer el texto del archivo descargado\n",
        "with open(path_to_file, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "print('Longitud del texto: {} caracteres'.format(len(text)))\n",
        "\n",
        "vocab = sorted(set(text))  # Vocabulario (todos los caracteres diferentes)\n",
        "print('Caracteres diferentes: {}'.format(len(vocab)))\n",
        "print('Vocabulario: ', vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lI2spVLZ9xO"
      },
      "source": [
        "### Tokenización e inversión del vocabulario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc5wYeuNZ9xO",
        "outputId": "d13dffa7-d6d3-4dd0-bff1-9feb3ed197ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'\\n'   --->    0\n",
            "' '    --->    1\n",
            "'!'    --->    2\n",
            "'('    --->    3\n",
            "')'    --->    4\n",
            "','    --->    5\n",
            "'-'    --->    6\n",
            "'.'    --->    7\n",
            "'/'    --->    8\n",
            "'0'    --->    9\n",
            "'1'    --->   10\n",
            "'2'    --->   11\n",
            "'3'    --->   12\n",
            "'4'    --->   13\n",
            "'5'    --->   14\n",
            "'6'    --->   15\n",
            "'7'    --->   16\n",
            "'8'    --->   17\n",
            "'9'    --->   18\n",
            "':'    --->   19\n",
            "';'    --->   20\n",
            "'?'    --->   21\n",
            "'A'    --->   22\n",
            "'B'    --->   23\n",
            "'C'    --->   24\n",
            "'D'    --->   25\n",
            "'E'    --->   26\n",
            "'F'    --->   27\n",
            "'G'    --->   28\n",
            "'H'    --->   29\n",
            "'I'    --->   30\n",
            "'J'    --->   31\n",
            "'K'    --->   32\n",
            "'L'    --->   33\n",
            "'M'    --->   34\n",
            "'N'    --->   35\n",
            "'O'    --->   36\n",
            "'P'    --->   37\n",
            "'Q'    --->   38\n",
            "'R'    --->   39\n",
            "'S'    --->   40\n",
            "'T'    --->   41\n",
            "'U'    --->   42\n",
            "'V'    --->   43\n",
            "'W'    --->   44\n",
            "'X'    --->   45\n",
            "'Y'    --->   46\n",
            "'Z'    --->   47\n",
            "'['    --->   48\n",
            "']'    --->   49\n",
            "'a'    --->   50\n",
            "'b'    --->   51\n",
            "'c'    --->   52\n",
            "'d'    --->   53\n",
            "'e'    --->   54\n",
            "'f'    --->   55\n",
            "'g'    --->   56\n",
            "'h'    --->   57\n",
            "'i'    --->   58\n",
            "'j'    --->   59\n",
            "'k'    --->   60\n",
            "'l'    --->   61\n",
            "'m'    --->   62\n",
            "'n'    --->   63\n",
            "'o'    --->   64\n",
            "'p'    --->   65\n",
            "'q'    --->   66\n",
            "'r'    --->   67\n",
            "'s'    --->   68\n",
            "'t'    --->   69\n",
            "'u'    --->   70\n",
            "'v'    --->   71\n",
            "'w'    --->   72\n",
            "'x'    --->   73\n",
            "'y'    --->   74\n",
            "'z'    --->   75\n",
            "'{'    --->   76\n",
            "'}'    --->   77\n",
            "'é'    --->   78\n",
            "'ê'    --->   79\n",
            "'—'    --->   80\n",
            "'‘'    --->   81\n",
            "'’'    --->   82\n",
            "'“'    --->   83\n",
            "'”'    --->   84\n",
            "'…'    --->   85\n"
          ]
        }
      ],
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)} # diccionario que asigna un índice a cada caracter\n",
        "\n",
        "idx2char = np.array(vocab) # array que asigna un caracter a cada índice (el inverso del anterior)\n",
        "\n",
        "for char, _ in zip(char2idx, range(len(vocab))):\n",
        "    print('{:6s} ---> {:4d}'.format(repr(char), char2idx[char]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW8_NrFCZ9xO"
      },
      "source": [
        "### Conversión de texto a secuencias de enteros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HntCTSgYZ9xP",
        "outputId": "1136bceb-3896-4577-8621-1fbeba558d3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto:  '\\n\\n“We should start back,” Gared urged as the woods' ...\n",
            "Codificado:  [ 0  0 83 44 54  1 68 57 64 70 61 53  1 68 69 50 67 69  1 51 50 52 60  5\n",
            " 84  1 28 50 67 54 53  1 70 67 56 54 53  1 50 68  1 69 57 54  1 72 64 64\n",
            " 53 68]\n"
          ]
        }
      ],
      "source": [
        "text_as_int = np.array([char2idx[c] for c in text]) # array que asigna un índice a cada caracter del texto\n",
        "\n",
        "# Mostramos cómo se ha codificado el texto\n",
        "print ('Texto: ', format(repr(text[:50])), '...'\n",
        "       '\\nCodificado: ', text_as_int[:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu4P9ntQZ9xP"
      },
      "source": [
        "### Preparación de los datos de entrenamiento y prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YYhgDc25Z9xP"
      },
      "outputs": [],
      "source": [
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) # creamos un dataset con los índices del texto (texto codificado)\n",
        "\n",
        "seq_length = 100 # longitud de la secuencia de entrada (número de caracteres que se le pasan a la red)\n",
        "\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True) # dividimos el dataset en secuencias de longitud seq_length+1 porque la red predice el siguiente caracter a partir de los seq_length anteriores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7bKoTQLZ9xP",
        "outputId": "29e7c3e7-b845-4888-c865-69e51c756e9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'\\n\\n“We should start back,” Gared urged as the woods began to grow dark around them. “The wildlings are'\n",
            "' dead.”\\n\\n“Do the dead frighten you?” Ser Waymar Royce asked with just the hint of a smile.\\n\\nGared did'\n",
            "' not rise to the bait. He was an old man, past fifty, and he had seen the lordlings come and go. “Dea'\n",
            "'d is dead,” he said. “We have no business with the dead.”\\n\\n“Are they dead?” Royce asked softly. “What'\n",
            "' proof have we?”\\n\\n“Will saw them,” Gared said. “If he says they are dead, that’s proof enough for me.'\n"
          ]
        }
      ],
      "source": [
        "# Comprobación de que se ha dividido correctamente\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8ur26ad-Z9xP"
      },
      "outputs": [],
      "source": [
        "# Preparación de datos de entrenamiento\n",
        "# La entrada corresponde al caracter 0 hasta el caracter 99\n",
        "# La salida corresponde al caracter 1 hasta el caracter 100\n",
        "\n",
        "def split_input_target(chunk): # función que separa la entrada de la salida\n",
        "  input_text = chunk[:-1] # entrada\n",
        "  target_text = chunk[1:] # salida\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target) # aplicamos la función anterior a cada secuencia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS-IEnNqZ9xP",
        "outputId": "e7dde490-6d96-402d-cd80-f396489bd72c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "# Agrupamiento de los datos en lotes de tamaño 64\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000   # Tamaño del buffer para barajar\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True) # Drop_remainder=True para que todos los lotes tengan el mismo tamaño, los lotes que no se alcancen a completar se descartan\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NX6RyxLZ9xP"
      },
      "source": [
        "### Definición del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0KLSKaN6Z9xP"
      },
      "outputs": [],
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size): # función que crea el modelo\n",
        "  model = tf.keras.Sequential([\n",
        "    layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]), # capa de embedding\n",
        "    layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'), # capa LSTM\n",
        "    layers.Dense(vocab_size) # capa densa\n",
        "  ])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4ME-8GZZ9xP",
        "outputId": "914b0a9e-1a05-4209-ef45-cc8958de43cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           22016     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 86)            88150     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5357142 (20.44 MB)\n",
            "Trainable params: 5357142 (20.44 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(vocab) # tamaño del vocabulario\n",
        "embedding_dim = 256 # dimensión del embedding\n",
        "rnn_units = 1024 # número de neuronas de la capa LSTM\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE) # construimos el modelo\n",
        "\n",
        "model.summary() # mostramos un resumen del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-mr-na8Z9xP"
      },
      "source": [
        "### Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "H2jSYQXPZ9xP"
      },
      "outputs": [],
      "source": [
        "def loss(labels, logits): # función de pérdida\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vgu3Lf8HZ9xP"
      },
      "outputs": [],
      "source": [
        "# Compilación del modelo\n",
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WX_pQ4X0Z9xP"
      },
      "outputs": [],
      "source": [
        "# Añadir checkpoints para almacenar los pesos del modelo en cada época\n",
        "\n",
        "checkpoint_dir = './training_checkpoints' # directorio donde se guardarán los checkpoints\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\") # nombre de los archivos de los checkpoints\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "  filepath=checkpoint_prefix,\n",
        "  save_weights_only=True) # sólo se guardan los pesos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZY1aKkgZ9xP",
        "outputId": "3de2d057-0f92-4572-e88b-78257327cdc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "875/875 [==============================] - 71s 65ms/step - loss: 1.7547\n",
            "Epoch 2/50\n",
            "875/875 [==============================] - 60s 66ms/step - loss: 1.2940\n",
            "Epoch 3/50\n",
            "875/875 [==============================] - 66s 71ms/step - loss: 1.2137\n",
            "Epoch 4/50\n",
            "875/875 [==============================] - 64s 71ms/step - loss: 1.1709\n",
            "Epoch 5/50\n",
            "875/875 [==============================] - 64s 71ms/step - loss: 1.1408\n",
            "Epoch 6/50\n",
            "875/875 [==============================] - 64s 72ms/step - loss: 1.1162\n",
            "Epoch 7/50\n",
            "875/875 [==============================] - 65s 73ms/step - loss: 1.0956\n",
            "Epoch 8/50\n",
            "875/875 [==============================] - 65s 73ms/step - loss: 1.0774\n",
            "Epoch 9/50\n",
            "875/875 [==============================] - 65s 73ms/step - loss: 1.0610\n",
            "Epoch 10/50\n",
            "875/875 [==============================] - 65s 72ms/step - loss: 1.0462\n",
            "Epoch 11/50\n",
            "875/875 [==============================] - 64s 71ms/step - loss: 1.0326\n",
            "Epoch 12/50\n",
            "875/875 [==============================] - 64s 71ms/step - loss: 1.0204\n",
            "Epoch 13/50\n",
            "875/875 [==============================] - 64s 71ms/step - loss: 1.0093\n",
            "Epoch 14/50\n",
            "875/875 [==============================] - 65s 73ms/step - loss: 0.9994\n",
            "Epoch 15/50\n",
            "875/875 [==============================] - 64s 71ms/step - loss: 0.9905\n",
            "Epoch 16/50\n",
            "875/875 [==============================] - 65s 73ms/step - loss: 0.9827\n",
            "Epoch 17/50\n",
            "875/875 [==============================] - 64s 71ms/step - loss: 0.9756\n",
            "Epoch 18/50\n",
            "875/875 [==============================] - 64s 71ms/step - loss: 0.9696\n",
            "Epoch 19/50\n",
            "875/875 [==============================] - 64s 72ms/step - loss: 0.9646\n",
            "Epoch 20/50\n",
            "875/875 [==============================] - 65s 72ms/step - loss: 0.9601\n",
            "Epoch 21/50\n",
            "875/875 [==============================] - 65s 73ms/step - loss: 0.9565\n",
            "Epoch 22/50\n",
            "875/875 [==============================] - 64s 71ms/step - loss: 0.9532\n",
            "Epoch 23/50\n",
            "875/875 [==============================] - 65s 72ms/step - loss: 0.9507\n",
            "Epoch 24/50\n",
            "875/875 [==============================] - 65s 72ms/step - loss: 0.9487\n",
            "Epoch 25/50\n",
            "875/875 [==============================] - 66s 73ms/step - loss: 0.9478\n",
            "Epoch 26/50\n",
            "875/875 [==============================] - 65s 73ms/step - loss: 0.9473\n",
            "Epoch 27/50\n",
            "875/875 [==============================] - 65s 73ms/step - loss: 0.9462\n",
            "Epoch 28/50\n",
            "875/875 [==============================] - 64s 71ms/step - loss: 0.9464\n",
            "Epoch 29/50\n",
            "875/875 [==============================] - 64s 71ms/step - loss: 0.9467\n",
            "Epoch 30/50\n",
            "875/875 [==============================] - 65s 73ms/step - loss: 0.9481\n",
            "Epoch 31/50\n",
            "875/875 [==============================] - 64s 71ms/step - loss: 0.9490\n",
            "Epoch 32/50\n",
            "875/875 [==============================] - 65s 73ms/step - loss: 0.9507\n",
            "Epoch 33/50\n",
            "875/875 [==============================] - 66s 73ms/step - loss: 0.9529\n",
            "Epoch 34/50\n",
            "875/875 [==============================] - 65s 73ms/step - loss: 0.9540\n",
            "Epoch 35/50\n",
            "875/875 [==============================] - 65s 73ms/step - loss: 0.9564\n",
            "Epoch 36/50\n",
            "875/875 [==============================] - 64s 71ms/step - loss: 0.9581\n",
            "Epoch 37/50\n",
            "875/875 [==============================] - 65s 73ms/step - loss: 0.9624\n",
            "Epoch 38/50\n",
            "875/875 [==============================] - 66s 73ms/step - loss: 0.9638\n",
            "Epoch 39/50\n",
            "875/875 [==============================] - 64s 72ms/step - loss: 0.9670\n",
            "Epoch 40/50\n",
            "875/875 [==============================] - 64s 71ms/step - loss: 0.9729\n",
            "Epoch 41/50\n",
            "875/875 [==============================] - 65s 73ms/step - loss: 0.9763\n",
            "Epoch 42/50\n",
            "875/875 [==============================] - 64s 72ms/step - loss: 0.9800\n",
            "Epoch 43/50\n",
            "875/875 [==============================] - 65s 73ms/step - loss: 0.9833\n",
            "Epoch 44/50\n",
            "875/875 [==============================] - 64s 71ms/step - loss: 0.9865\n",
            "Epoch 45/50\n",
            "875/875 [==============================] - 64s 71ms/step - loss: 0.9936\n",
            "Epoch 46/50\n",
            "875/875 [==============================] - 65s 73ms/step - loss: 0.9975\n",
            "Epoch 47/50\n",
            "875/875 [==============================] - 66s 73ms/step - loss: 1.0035\n",
            "Epoch 48/50\n",
            "875/875 [==============================] - 64s 71ms/step - loss: 1.0103\n",
            "Epoch 49/50\n",
            "875/875 [==============================] - 66s 73ms/step - loss: 1.0147\n",
            "Epoch 50/50\n",
            "875/875 [==============================] - 64s 71ms/step - loss: 1.0194\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e7fe24e6d40>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "EPOCHS = 50 # número de épocas\n",
        "model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback]) # entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr_XP9P6Z9xP"
      },
      "source": [
        "### Generación de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xFA2BS0lZ9xP"
      },
      "outputs": [],
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1) # construimos el modelo con batch_size=1\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)) # cargamos los pesos del último checkpoint\n",
        "\n",
        "model.build(tf.TensorShape([1, None])) # construimos el modelo con batch_size=1, el 1 es porque solo se espera 1 oración de entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "pjF-vs_jZ9xQ"
      },
      "outputs": [],
      "source": [
        "# Función para generar texto caracter a caracter\n",
        "\n",
        "def generate_text(model, start_string):\n",
        "  num_generate = 500 # número de caracteres a generar\n",
        "  input_eval = [char2idx[s] for s in start_string] # convertimos la cadena de entrada en una lista de índices (números)\n",
        "\n",
        "  input_eval = tf.expand_dims(input_eval, 0) # añadimos una dimensión al principio (batch_size=1)\n",
        "\n",
        "  text_generated = [] # lista para almacenar el texto generado\n",
        "\n",
        "  temperature = 0.9 # parámetro para controlar la aleatoriedad de la predicción (0 = predicción determinista, 1 = predicción aleatoria)\n",
        "\n",
        "  model.reset_states() # reiniciamos el estado de la red\n",
        "\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval) # predicciones de la red\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / temperature  # dividimos entre temperature para controlar la aleatoriedad de la predicción\n",
        "\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()  # obtenemos el índice de la predicción (el caracter predicho)\n",
        "                                                                                    # [-1,0] para obtener el último caracter predicho\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id]) # añadimos el caracter predicho a la lista\n",
        "\n",
        "  return (start_string + ''.join(text_generated)) # devolvemos la cadena de entrada + la cadena generada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMxx6BpsZ9xQ"
      },
      "source": [
        "### Resultados\n",
        "\n",
        "A continuación, analizaremos los resultados obtenidos en la generación de texto a partir de la cadena \"Dragon \" y una temperatura de 0.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6ky2ILYZ9xQ",
        "outputId": "ee4c0e0b-4336-4ec8-ed18-b78b341e7e2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dragon before he was so stupid as the others said. “I will not see the shape of your hands.”\n",
            "\n",
            "“I will not have to say that the sea of the water was stupid.” The old man shook his head. “I was a knight before I could find you.”\n",
            "\n",
            "“I would have to say that the girl was not about to say that you were a strong power to come to me.”\n",
            "\n",
            "“I will not have to say that the girl who was the best thing I have now. I was a knight, and the stars won’t be crowned and well as she was a man of the Night’s Watch. I was the\n"
          ]
        }
      ],
      "source": [
        "# Temperatura = 0.1\n",
        "print(generate_text(model, start_string=\"Dragon \")) # generamos texto a partir de la cadena \"Dragon \""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_UP19nOZ9xQ"
      },
      "source": [
        "Probamos el mismo input con una temperatura de 0.5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AiB-BO-Z9xQ",
        "outputId": "a1d56c89-ac09-48fd-e94f-efcbaf6c5d26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dragon showed a muddy drink of white scat. “The castle now,” he said, “but he was the bloody banner.”\n",
            "\n",
            "“We have to go to your castle all the other way.” He smiled at his father’s hands. “There was a bird, and the next morning that matters we married, and the north will be born a man as well. Lord Tywin sinse your lord father’s side.”\n",
            "\n",
            "“No.” He was pleased to see the sort of squire to his feet. “It was the bride of House Frey a whimpery, but I must ask them to let him stay a better life and a sword and \n"
          ]
        }
      ],
      "source": [
        "# Temperatura = 0.5\n",
        "print(generate_text(model, start_string=\"Dragon \")) # generamos texto a partir de la cadena \"Dragon \""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbqzAbNsZ9xQ"
      },
      "source": [
        "Probamos una última vez el mismo input con una temperatura de 0.9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfLRPk0-Z9xQ",
        "outputId": "996141c8-6252-4ec7-d29a-5c70cf0032e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dragon had been half-curious than he was, and there were almost penny, and his wishes were their father on Pyke. Prousemon remains at Harrenhal. A knight at the time he had the sort of maiden’s son, nd made him feel too late. The weeds reeled in the eyes of the e face of the highest table, but it was not like to be properly as he would answer.\n",
            "\n",
            "“My lord,” said Melisandre with his bloody hand. “I did not come with you.”\n",
            "\n",
            "“The Tyrells were cold and nubbled at the chaos once Ser Horas Redwyne may be pleas\n"
          ]
        }
      ],
      "source": [
        "# Temperatura = 0.9\n",
        "print(generate_text(model, start_string=\"Dragon \")) # generamos texto a partir de la cadena \"Dragon \""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl46HRm8Z9xQ"
      },
      "source": [
        "Por último, probamos con el input \"The \" y las mismas temperaturas antes mencionadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn7p-iGZZ9xQ",
        "outputId": "824d3656-cf3d-466b-8067-9ad1c4208c89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The will be the same as you can. The wildlings were true. The boy is the blood of the dragon, she thought. I was told to see the shapes of the window and let the fire had been so long as the star between them. The small brothers were so far as the river was blowing on the stone walls of the corner of his head. A slave was the one that might have been a strong sword in his hand. “I will not say that a lie to you.”\n",
            "\n",
            "“I would have to say that the girl who was the one who lived the point of his sworn sh\n"
          ]
        }
      ],
      "source": [
        "# Temperatura = 0.1\n",
        "print(generate_text(model, start_string=\"The \")) # generamos texto a partir de la cadena \"The \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9PTPAXfZ9xQ",
        "outputId": "6ec7e4c3-337b-4ba0-b34d-376d3484e60a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The watched the next day, and a third so loud and short and sorry for that. “I was soired a man he’d stay to the seven cry for his life. But the crown was still, his bright black hair covered with blood and broken spears. The wind blew on his back, twice, a bronze black beard, but a slope took him in the broken top of his shore. He had a short time before the warmth of his golden spears and a huge blade loose and then he could feel the steel on his legs. “I don’t even trust himself a septon proud, i\n"
          ]
        }
      ],
      "source": [
        "# Temperatura = 0.5\n",
        "print(generate_text(model, start_string=\"The \")) # generamos texto a partir de la cadena \"The \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBBB2T82Z9xQ",
        "outputId": "3afa1560-e656-4f62-b849-68b440764786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The white sus or die all the world, though. We all are more armed more yanding there are gone, don’t talk for to see what you believe every one? Sansa knew the man who died on her belly, then sounds well. It would not come to this masting thousand men. Mady of the Iron Throne is the Lord of Light have laughed at one of the day for a long most cruel way. Darry’s the Fish kings had decreed dead it down and form the Goat halls and fodder looked down at them with sweat. Jon sat swamoped under his arm, w\n"
          ]
        }
      ],
      "source": [
        "# Temperatura = 0.9\n",
        "print(generate_text(model, start_string=\"The \")) # generamos texto a partir de la cadena \"The \""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusiones\n",
        "\n",
        "De acuerdo con los resultados obtenidos, podemos concluir que la temperatura en el contexto del modelado de lenguaje, especialmente en modelos de generación de texto como el que estás utilizando, es un parámetro clave que controla la aleatoriedad en la generación de texto. Básicamente, afecta la diversidad y la creatividad en los resultados generados.\n",
        "\n",
        "- Una temperatura baja (0.1) produce resultados más deterministas, es decir, tiende a generar predicciones más seguras o probables basadas en el modelo. La salida es coherente y predecible, con repeticiones o patrones comunes.\n",
        "\n",
        "- Una temperatura media (0.5) aumenta la aleatoriedad en comparación con 0.1, lo que conduce a una mayor variedad en la elección de palabras y estructuras gramaticales. Los resultados pueden ser más creativos y menos predecibles, pero aún mantienen cierta coherencia.\n",
        "\n",
        "- Una temperatura alta (0.9) genera resultados altamente aleatorios y creativos. A menudo, produce texto más incoherente, con fragmentos que pueden carecer de sentido lógico, pero que aportan una gran dosis de creatividad. Las frases tienden a ser menos predecibles y coherentes.\n",
        "\n",
        "En resumen, a medida que la temperatura aumenta, la aleatoriedad y la creatividad en los textos generados también aumentan, pero a expensas de la coherencia y la lógica en el texto. A temperaturas más bajas, los resultados tienden a ser más coherentes y predecibles, basados en patrones más sólidos aprendidos por el modelo en el corpus (el libro importado en un inicio). Por ello, es importante elegir la temperatura según el propósito de la generación de texto: coherencia versus creatividad."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
