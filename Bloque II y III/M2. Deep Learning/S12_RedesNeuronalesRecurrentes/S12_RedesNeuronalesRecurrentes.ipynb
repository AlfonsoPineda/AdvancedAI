{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sesión 12: Redes Neuronales Recurrentes\n",
    "\n",
    "---\n",
    "\n",
    "Jueves 2 de Noviembre de 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de texto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del texto: 203251 caracteres\n",
      "Caracteres diferentes: 92\n",
      "Vocabulario:  ['\\n', '\\r', ' ', '!', '\"', '#', '%', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xad', 'ÿ', 'Š', '‡', '…']\n"
     ]
    }
   ],
   "source": [
    "path_to_fileDL = tf.keras.utils.get_file('DL-Introduccion-practica-con-Keras-1a.txt', 'https://raw.githubusercontent.com/jorditorresBCN/Deep-Learning-Introduccion-practica-con-Keras/master/DeepLearning-Introduccion-practica-con-Keras-PRIMERA-PARTE.txt')\n",
    "\n",
    "text = open(path_to_fileDL, 'rb').read().decode(encoding='utf-8')  \n",
    "print ('Longitud del texto: {} caracteres'.format(len(text)))\n",
    "\n",
    "vocab = sorted(set(text)) # vocabulario (todos los caracteres diferentes)\n",
    "print ('Caracteres diferentes: {}'.format(len(vocab))) \n",
    "\n",
    "print('Vocabulario: ', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización e inversión del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n'   --->    0\n",
      "'\\r'   --->    1\n",
      "' '    --->    2\n",
      "'!'    --->    3\n",
      "'\"'    --->    4\n",
      "'#'    --->    5\n",
      "'%'    --->    6\n",
      "\"'\"    --->    7\n",
      "'('    --->    8\n",
      "')'    --->    9\n",
      "'*'    --->   10\n",
      "'+'    --->   11\n",
      "','    --->   12\n",
      "'-'    --->   13\n",
      "'.'    --->   14\n",
      "'/'    --->   15\n",
      "'0'    --->   16\n",
      "'1'    --->   17\n",
      "'2'    --->   18\n",
      "'3'    --->   19\n",
      "'4'    --->   20\n",
      "'5'    --->   21\n",
      "'6'    --->   22\n",
      "'7'    --->   23\n",
      "'8'    --->   24\n",
      "'9'    --->   25\n",
      "':'    --->   26\n",
      "';'    --->   27\n",
      "'<'    --->   28\n",
      "'='    --->   29\n",
      "'>'    --->   30\n",
      "'?'    --->   31\n",
      "'@'    --->   32\n",
      "'A'    --->   33\n",
      "'B'    --->   34\n",
      "'C'    --->   35\n",
      "'D'    --->   36\n",
      "'E'    --->   37\n",
      "'F'    --->   38\n",
      "'G'    --->   39\n",
      "'H'    --->   40\n",
      "'I'    --->   41\n",
      "'J'    --->   42\n",
      "'K'    --->   43\n",
      "'L'    --->   44\n",
      "'M'    --->   45\n",
      "'N'    --->   46\n",
      "'O'    --->   47\n",
      "'P'    --->   48\n",
      "'Q'    --->   49\n",
      "'R'    --->   50\n",
      "'S'    --->   51\n",
      "'T'    --->   52\n",
      "'U'    --->   53\n",
      "'V'    --->   54\n",
      "'W'    --->   55\n",
      "'X'    --->   56\n",
      "'Y'    --->   57\n",
      "'['    --->   58\n",
      "']'    --->   59\n",
      "'_'    --->   60\n",
      "'a'    --->   61\n",
      "'b'    --->   62\n",
      "'c'    --->   63\n",
      "'d'    --->   64\n",
      "'e'    --->   65\n",
      "'f'    --->   66\n",
      "'g'    --->   67\n",
      "'h'    --->   68\n",
      "'i'    --->   69\n",
      "'j'    --->   70\n",
      "'k'    --->   71\n",
      "'l'    --->   72\n",
      "'m'    --->   73\n",
      "'n'    --->   74\n",
      "'o'    --->   75\n",
      "'p'    --->   76\n",
      "'q'    --->   77\n",
      "'r'    --->   78\n",
      "'s'    --->   79\n",
      "'t'    --->   80\n",
      "'u'    --->   81\n",
      "'v'    --->   82\n",
      "'w'    --->   83\n",
      "'x'    --->   84\n",
      "'y'    --->   85\n",
      "'z'    --->   86\n",
      "'\\xad' --->   87\n",
      "'ÿ'    --->   88\n",
      "'Š'    --->   89\n",
      "'‡'    --->   90\n",
      "'…'    --->   91\n"
     ]
    }
   ],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)} # diccionario que asigna un índice a cada caracter\n",
    "\n",
    "idx2char = np.array(vocab) # array que asigna un caracter a cada índice (el inverso del anterior)\n",
    "\n",
    "for char, _ in zip(char2idx, range(len(vocab))):\n",
    "    print('{:6s} ---> {:4d}'.format(repr(char), char2idx[char]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversión de texto a secuencias de enteros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto:  'Prologo\\r\\nEn 1953, Isaac Asimov publico Segunda Fun' ...\n",
      "Codificado:  [48 78 75 72 75 67 75  1  0 37 74  2 17 25 21 19 12  2 41 79 61 61 63  2\n",
      " 33 79 69 73 75 82  2 76 81 62 72 69 63 75  2 51 65 67 81 74 64 61  2 38\n",
      " 81 74]\n"
     ]
    }
   ],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text]) # array que asigna un índice a cada caracter del texto\n",
    "\n",
    "# Mostramos cómo se ha codificado el texto\n",
    "print ('Texto: ', format(repr(text[:50])), '...'\n",
    "       '\\nCodificado: ', text_as_int[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de los datos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) # creamos un dataset con los índices del texto (texto codificado)\n",
    "\n",
    "seq_length = 100 # longitud de la secuencia de entrada (número de caracteres que se le pasan a la red)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True) # dividimos el dataset en secuencias de longitud seq_length+1 porque la red predice el siguiente caracter a partir de los seq_length anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Prologo\\r\\nEn 1953, Isaac Asimov publico Segunda Fundacion, el tercer libro de la saga de la Fundacion '\n",
      "'(o el decimotercero segun otras fuentes, este es un tema de debate). En Segunda Fundacion aparece por'\n",
      "' primera vez Arkady Darell, uno de los principales personajes de la parte final de la saga. En su pri'\n",
      "'mera escena, Arkady, que tiene 14 anos, esta haciendo sus tareas escolares. En concreto, una redaccio'\n",
      "'n que lleva por titulo ?El Futuro del Plan Sheldon?. Para hacer la redaccion, Arkady esta utilizando '\n"
     ]
    }
   ],
   "source": [
    "# Comprobación de que se ha dividido correctamente\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparación de datos de entrenamiento\n",
    "# La entrada corresponde al caracter 0 hasta el caracter 99\n",
    "# La salida corresponde al caracter 1 hasta el caracter 100\n",
    "\n",
    "def split_input_target(chunk): # función que separa la entrada de la salida\n",
    "  input_text = chunk[:-1] # entrada\n",
    "  target_text = chunk[1:] # salida\n",
    "  return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target) # aplicamos la función anterior a cada secuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Agrupamiento de los datos en lotes de tamaño 64\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000   # Tamaño del buffer para barajar\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True) # Drop_remainder=True para que todos los lotes tengan el mismo tamaño, los lotes que no se alcancen a completar se descartan\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size): # función que crea el modelo\n",
    "  model = tf.keras.Sequential([\n",
    "    layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]), # capa de embedding\n",
    "    layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'), # capa LSTM\n",
    "    layers.Dense(vocab_size) # capa densa\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           23552     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 92)            94300     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5364828 (20.47 MB)\n",
      "Trainable params: 5364828 (20.47 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab) # tamaño del vocabulario\n",
    "embedding_dim = 256 # dimensión del embedding\n",
    "rnn_units = 1024 # número de neuronas de la capa LSTM\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE) # construimos el modelo\n",
    "\n",
    "model.summary() # mostramos un resumen del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits): # función de pérdida\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilación del modelo\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir checkpoints para almacenar los pesos del modelo en cada época\n",
    "\n",
    "checkpoint_dir = './training_checkpoints' # directorio donde se guardarán los checkpoints\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\") # nombre de los archivos de los checkpoints\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "  filepath=checkpoint_prefix,\n",
    "  save_weights_only=True) # sólo se guardan los pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 2.8470\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 2.4404\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 31s 998ms/step - loss: 2.2153\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 31s 998ms/step - loss: 2.0782\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 34s 1s/step - loss: 1.9457\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 31s 989ms/step - loss: 1.8158\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 1.6931\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 31s 1s/step - loss: 1.5786\n",
      "Epoch 9/50\n",
      "31/31 [==============================] - 31s 996ms/step - loss: 1.4791\n",
      "Epoch 10/50\n",
      "31/31 [==============================] - 31s 1s/step - loss: 1.3938\n",
      "Epoch 11/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 1.3119\n",
      "Epoch 12/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 1.2433\n",
      "Epoch 13/50\n",
      "31/31 [==============================] - 33s 1s/step - loss: 1.1852\n",
      "Epoch 14/50\n",
      "31/31 [==============================] - 31s 1s/step - loss: 1.1346\n",
      "Epoch 15/50\n",
      "31/31 [==============================] - 33s 1s/step - loss: 1.0798\n",
      "Epoch 16/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 1.0338\n",
      "Epoch 17/50\n",
      "31/31 [==============================] - 33s 1s/step - loss: 0.9935\n",
      "Epoch 18/50\n",
      "31/31 [==============================] - 33s 1s/step - loss: 0.9498\n",
      "Epoch 19/50\n",
      "31/31 [==============================] - 34s 1s/step - loss: 0.9060\n",
      "Epoch 20/50\n",
      "31/31 [==============================] - 36s 1s/step - loss: 0.8675\n",
      "Epoch 21/50\n",
      "31/31 [==============================] - 34s 1s/step - loss: 0.8252\n",
      "Epoch 22/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.7869\n",
      "Epoch 23/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.7498\n",
      "Epoch 24/50\n",
      "31/31 [==============================] - 33s 1s/step - loss: 0.7078\n",
      "Epoch 25/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.6713\n",
      "Epoch 26/50\n",
      "31/31 [==============================] - 31s 1s/step - loss: 0.6362\n",
      "Epoch 27/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.5999\n",
      "Epoch 28/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.5637\n",
      "Epoch 29/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.5329\n",
      "Epoch 30/50\n",
      "31/31 [==============================] - 33s 1s/step - loss: 0.4999\n",
      "Epoch 31/50\n",
      "31/31 [==============================] - 33s 1s/step - loss: 0.4686\n",
      "Epoch 32/50\n",
      "31/31 [==============================] - 34s 1s/step - loss: 0.4463\n",
      "Epoch 33/50\n",
      "31/31 [==============================] - 33s 1s/step - loss: 0.4211\n",
      "Epoch 34/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.3979\n",
      "Epoch 35/50\n",
      "31/31 [==============================] - 34s 1s/step - loss: 0.3763\n",
      "Epoch 36/50\n",
      "31/31 [==============================] - 33s 1s/step - loss: 0.3597\n",
      "Epoch 37/50\n",
      "31/31 [==============================] - 33s 1s/step - loss: 0.3454\n",
      "Epoch 38/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.3304\n",
      "Epoch 39/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.3164\n",
      "Epoch 40/50\n",
      "31/31 [==============================] - 31s 991ms/step - loss: 0.3045\n",
      "Epoch 41/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.2963\n",
      "Epoch 42/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.2858\n",
      "Epoch 43/50\n",
      "31/31 [==============================] - 35s 1s/step - loss: 0.2769\n",
      "Epoch 44/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.2696\n",
      "Epoch 45/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.2602\n",
      "Epoch 46/50\n",
      "31/31 [==============================] - 31s 1s/step - loss: 0.2573\n",
      "Epoch 47/50\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.2516\n",
      "Epoch 48/50\n",
      "31/31 [==============================] - 35s 1s/step - loss: 0.2464\n",
      "Epoch 49/50\n",
      "31/31 [==============================] - 33s 1s/step - loss: 0.2449\n",
      "Epoch 50/50\n",
      "31/31 [==============================] - 34s 1s/step - loss: 0.2380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x287b72b20>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 50 # número de épocas\n",
    "model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback]) # entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1) # construimos el modelo con batch_size=1\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)) # cargamos los pesos del último checkpoint\n",
    "\n",
    "model.build(tf.TensorShape([1, None])) # construimos el modelo con batch_size=1, el 1 es porque solo se espera 1 oración de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para generar texto caracter a caracter\n",
    "\n",
    "def generate_text(model, start_string):\n",
    "  num_generate = 500 # número de caracteres a generar\n",
    "  input_eval = [char2idx[s] for s in start_string] # convertimos la cadena de entrada en una lista de índices (números)\n",
    "\n",
    "  input_eval = tf.expand_dims(input_eval, 0) # añadimos una dimensión al principio (batch_size=1)\n",
    "\n",
    "  text_generated = [] # lista para almacenar el texto generado\n",
    "\n",
    "  temperature = 0.5 # parámetro para controlar la aleatoriedad de la predicción (0 = predicción determinista, 1 = predicción aleatoria)\n",
    "\n",
    "  model.reset_states() # reiniciamos el estado de la red\n",
    "\n",
    "  for i in range(num_generate):\n",
    "    predictions = model(input_eval) # predicciones de la red\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "    predictions = predictions / temperature  # dividimos entre temperature para controlar la aleatoriedad de la predicción\n",
    "\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()  # obtenemos el índice de la predicción (el caracter predicho)\n",
    "                                                                                    # [-1,0] para obtener el último caracter predicho\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    text_generated.append(idx2char[predicted_id]) # añadimos el caracter predicho a la lista\n",
    "\n",
    "  return (start_string + ''.join(text_generated)) # devolvemos la cadena de entrada + la cadena generada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redes neuronales convolucionales, muy usadas en tareas de vision por computador. \n",
      "Las redes neuronales convolucionales avon con el lector, programaremos una red neuronal convolucional nos permite conseguir una entrada adicional fija de x0=1).\n",
      "En la fase de entrenamiento de un modelo consideremos que tenemos un buen resultado de clasificacion:\n",
      "\n",
      "\n",
      "\n",
      "Si lo ha probado el lector, en este caso supongo que obtenga el valor de estos parametros examinando el momento de los cientificos y cientificas de dato\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"redes \")) # generamos texto a partir de la cadena \"redes \""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
